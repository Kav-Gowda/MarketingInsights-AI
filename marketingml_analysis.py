# -*- coding: utf-8 -*-
"""MarketingML_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aaNZQutsU5bS7kQlQjl_TQfZaKqLNMex
"""

# --- Install dependencies ---
!pip install -q pandas numpy matplotlib seaborn scikit-learn statsmodels tabulate tensorflow

# --- Imports ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from statsmodels.graphics.tsaplots import acf, pacf, plot_acf, plot_pacf
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn import metrics

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM
from tensorflow.keras.callbacks import EarlyStopping

# --- Load datasets (local files; Colab upload fallback) ---
# Expected filenames (use your local copies):
#  - "Features data set.csv"
#  - "sales data-set.csv"
#  - "stores data-set.csv"

import os

def read_csv_robust(path):
    for enc in ("utf-8", "latin-1", "iso-8859-1"):
        try:
            return pd.read_csv(path, delimiter=',', encoding=enc)
        except Exception:
            continue
    raise ValueError(f"Could not read {path} with common encodings.")

needed_files = ["Features data set.csv", "sales data-set.csv", "stores data-set.csv"]
missing = [f for f in needed_files if not os.path.exists(f)]

if missing:
    try:
        from google.colab import files
        print("Please upload the following files:", missing)
        files.upload()  # choose the three CSVs from your machine
    except Exception:
        print("If not using Colab, place the CSV files in the working directory.")

df1 = read_csv_robust("Features data set.csv")
df1.dataframeName = "Features data set.csv"

df2 = read_csv_robust("sales data-set.csv")
df2.dataframeName = "Sales data set.csv"

df3 = read_csv_robust("stores data-set.csv")
df3.dataframeName = "Stores data set.csv"

# Quick sanity checks
print("df1:", df1.shape, "| columns:", len(df1.columns))
print("df2:", df2.shape, "| columns:", len(df2.columns))
print("df3:", df3.shape, "| columns:", len(df3.columns))

# --- Merge and clean datasets ---
df = df1.merge(df3, on='Store')
df = df2.merge(df, on=['Store', 'Date', 'IsHoliday'])

# Handle missing values and correct data types
df = df.fillna(0)
df['Date'] = pd.to_datetime(df['Date'])
df['Type'] = df['Type'].astype('category')

print(f"✅ Merged dataset shape: {df.shape[0]} rows × {df.shape[1]} columns")
df.info()

# --- Explore and isolate a specific department ---
# Group to verify structure and count rows per department
dept_counts = df[['Store', 'Dept']].value_counts().sort_values(ascending=False)
print("Top departments by record count:\n", dept_counts.head(10))

# Focus on a single store & department for forecasting
St, Dt = 24, 50
df_d = df[(df['Store'] == St) & (df['Dept'] == Dt)].sort_values('Date')

print(f"\n✅ Subset created for Store {St}, Dept {Dt} | Rows: {df_d.shape[0]}")
df_d.head()

# --- Index & column selection ---
df_d = df_d.set_index('Date')
df_d = df_d[['Weekly_Sales','IsHoliday','Temperature','Fuel_Price',
             'MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5',
             'CPI','Unemployment']]
df_d.head()

# --- Correlation heatmap helper + view ---
def my_headmap(corr):
    """
    corr: correlation matrix (DataFrame)
    """
    mask = np.triu(np.ones_like(corr, dtype=bool))
    f, ax = plt.subplots(figsize=(11, 9))
    sns.heatmap(corr, mask=mask, cmap='RdYlGn', vmin=-1., vmax=1., annot=True, center=0,
                square=True, linewidths=.5, cbar_kws={"shrink": .5})
    plt.show()

my_headmap(df_d.corr())

# --- Join with 4-week lag features (build if missing) ---
# Expect a supervised "dataset" with lag columns already present.
# If not present, create from Weekly_Sales with 4 lags.
if 'dataset' not in globals():
    def series_to_supervised(in_data, tar_data, n_in=1, dropnan=True, target_dep=False):
        n_vars = in_data.shape[1]
        cols, names = [], []
        i_start = 1 if target_dep else 0
        for i in range(i_start, n_in + 1):
            cols.append(in_data.shift(i))
            names += [f"{in_data.columns[j]}(t-{i})" for j in range(n_vars)]
        if target_dep:
            for i in range(n_in, -1, -1):
                cols.append(tar_data.shift(i))
                names += [f"{tar_data.name}(t-{i})"]
        else:
            cols.append(tar_data)
            names.append(tar_data.name)
        agg = pd.concat(cols, axis=1)
        agg.columns = names
        if dropnan:
            agg.dropna(inplace=True)
        return agg

    dataset = series_to_supervised(pd.DataFrame(df_d['Weekly_Sales']), df_d['Weekly_Sales'], n_in=4, dropnan=True)

df_hp = df_d.join(dataset[dataset.columns[1:-1]]).dropna()
df_hp.head()

# --- Build X, Y and scale ---
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

col = df_hp.columns
X, Y = df_hp[col[1:]], df_hp[col[0]]

scaler_x = MinMaxScaler(feature_range=(0, 1))
scaler_y = MinMaxScaler(feature_range=(0, 1))

scaled_x = scaler_x.fit_transform(X)
scaled_y = scaler_y.fit_transform(Y.values.reshape(-1, 1))

x_train, x_test, y_train, y_test = train_test_split(scaled_x, scaled_y, test_size=0.3, shuffle=False)

# Real-scale references
res_train = scaler_y.inverse_transform(y_train).flatten()
res_test  = scaler_y.inverse_transform(y_test).flatten()

# --- Linear model baseline ---
from sklearn.linear_model import LinearRegression
from sklearn import metrics

regressor = LinearRegression()
regressor.fit(x_train, y_train)

y_pred_test_ln = regressor.predict(x_test)
y_pred_test_ln = scaler_y.inverse_transform(y_pred_test_ln).flatten()

print("R^2 train (scaled):", regressor.score(x_train, y_train))
print("R^2 test  (scaled):", regressor.score(x_test, y_test))
print('MAE (real):',  metrics.mean_absolute_error(res_test, y_pred_test_ln))
print('MSE (real):',  metrics.mean_squared_error(res_test, y_pred_test_ln))
print('RMSE (real):', np.sqrt(metrics.mean_squared_error(res_test, y_pred_test_ln)))

# --- Backprop NN (Keras) with EarlyStopping ---
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

def BP_model(X):
    model = Sequential()
    model.add(Dense(100, input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(50, kernel_initializer='normal', activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(1, kernel_initializer='normal'))
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

epochs = 1000
batch_size = max(8, int(y_train.shape[0]*.1))

bp_model = BP_model(x_train)
es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

history = bp_model.fit(
    x_train, y_train,
    validation_data=(x_test, y_test),
    epochs=epochs,
    batch_size=batch_size,
    verbose=0,
    callbacks=[es]
)

# Plot loss
plt.figure()
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.ylabel('loss'); plt.xlabel('epoch'); plt.legend(); plt.show()

# Predictions (inverse-transform)
res_tr = bp_model.predict(x_train, verbose=0)
res_ts = bp_model.predict(x_test,  verbose=0)
res_train_ANN = scaler_y.inverse_transform(res_tr.reshape(-1, 1)).flatten()
res_test_ANN  = scaler_y.inverse_transform(res_ts.reshape(-1, 1)).flatten()

# Compare
print("Correlation train:", np.corrcoef(res_train, res_train_ANN)[0,1])
print("Correlation test: ", np.corrcoef(res_test,  res_test_ANN)[0,1])
print('MAE (real):',  metrics.mean_absolute_error(res_test, res_test_ANN))
print('MSE (real):',  metrics.mean_squared_error(res_test,  res_test_ANN))
print('RMSE (real):', np.sqrt(metrics.mean_squared_error(res_test, res_test_ANN)))

# --- Visualization: Actual vs Predictions (Linear vs ANN) ---
res_pred_test_ln  = pd.Series(y_pred_test_ln,  name='Predicted test Linear Model')
res_pred_test_ANN = pd.Series(res_test_ANN,    name='Predicted test ANN')

df_2 = pd.DataFrame({'Actual test': res_test,
                     'Linear Model': res_pred_test_ln,
                     'ANN Model': res_pred_test_ANN})
df_2.index = df_d.index[len(df_d)-len(res_test):]

df_2.plot(title='Test: Actual vs Predicted (Linear vs ANN)', figsize=(12,6))
plt.show()

# --- Sensitivity analysis helpers ---
def my_sens(model_like, x, c, p):
    """
    x: numpy array of inputs
    c: column index to perturb
    p: fractional change (e.g., 0.1 for +10%)
    Returns fractional change in prediction on the last row.
    """
    X = x[-1:].copy()
    y_pred = model_like.predict(X)
    X[:, c] = X[:, c] * (1 + p)
    y_pred_delta = model_like.predict(X)
    return ((y_pred_delta - y_pred) / y_pred)[0][0] if hasattr(y_pred, "__len__") else (y_pred_delta - y_pred)/y_pred

print("Sensitivity of Week Sales (last point) with +10% change in each factor:")
for i, c in enumerate(df_hp.columns[1:]):  # skip target
    print(f"- {c:>15}: {my_sens(bp_model, x_test, i, 0.1)*100:6.2f}%")

# --- Sensitivity on holiday-only subset ---
x_test2 = np.array([row for row in x_test if row[0] >= 0.99])  # assumes IsHoliday normalized ~[0,1]
if x_test2.size:
    print("\nSensitivity during Holiday weeks (+10% factor change):")
    for i, c in enumerate(df_hp.columns[1:]):
        print(f"- {c:>15}: {my_sens(bp_model, x_test2, i, 0.1)*100:6.2f}%")
else:
    print("\nNo holiday rows in test subset after normalization filter.")



